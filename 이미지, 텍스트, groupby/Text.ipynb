{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':['I like kaggle very much',\n",
    "                           'I do not like kaggle',\n",
    "                           'I do really love machine learning']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장을 벡터로 변환 (특징 부여)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 수 세기 'bag of words'\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "bag= vectorizer.fit_transform(df['text'])\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'like': 4,\n",
       " 'kaggle': 2,\n",
       " 'very': 10,\n",
       " 'much': 7,\n",
       " 'do': 0,\n",
       " 'not': 8,\n",
       " 'really': 9,\n",
       " 'love': 5,\n",
       " 'machine': 6,\n",
       " 'learning': 3}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_ #각 단어의 index번호"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag of words의 장점: 이해하기 쉬움\n",
    "\n",
    "단점:\n",
    "\n",
    "- 특별한 의미 x\n",
    "\n",
    "- 단어끼리의 인접성 확인 불가\n",
    "\n",
    "- 단어 순서와 관련된 정보 x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1번 단점 해결 대책 : TF-IDF \"희귀성을 고려\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.31544415 0.40619178 0.         0.40619178 0.\n",
      "  0.         0.53409337 0.         0.         0.53409337]\n",
      " [0.43306685 0.33631504 0.43306685 0.         0.43306685 0.\n",
      "  0.         0.         0.56943086 0.         0.        ]\n",
      " [0.34261996 0.26607496 0.         0.45050407 0.         0.45050407\n",
      "  0.45050407 0.         0.         0.45050407 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer=CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "transformer=TfidfTransformer()\n",
    "\n",
    "tf= vectorizer.fit_transform(df['text'])\n",
    "tfidf=transformer.fit_transform(tf)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.31544415103177975, 0.4061917781433947, 0.0, 0.4061917781433947, 0.0, 0.0, 0.5340933749435834, 0.0, 0.0, 0.5340933749435834]\n",
      "[0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "index 비교\n",
      "tfid:  0.0\n",
      "tf:  0\n",
      "------------\n",
      "tfid:  0.31544415103177975\n",
      "tf:  1\n",
      "------------\n",
      "tfid:  0.4061917781433947\n",
      "tf:  1\n",
      "------------\n",
      "tfid:  0.0\n",
      "tf:  0\n",
      "------------\n",
      "tfid:  0.4061917781433947\n",
      "tf:  1\n",
      "------------\n",
      "tfid:  0.0\n",
      "tf:  0\n",
      "------------\n",
      "tfid:  0.0\n",
      "tf:  0\n",
      "------------\n",
      "tfid:  0.5340933749435834\n",
      "tf:  1\n",
      "------------\n",
      "tfid:  0.0\n",
      "tf:  0\n",
      "------------\n",
      "tfid:  0.0\n",
      "tf:  0\n",
      "------------\n",
      "tfid:  0.5340933749435834\n",
      "tf:  1\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "print(list(tfidf.toarray()[0]))\n",
    "print(list(tf.toarray()[0]))\n",
    "\n",
    "print('index 비교')\n",
    "for i, j in zip(list(tfidf.toarray()[0]), list(tf.toarray()[0])):\n",
    "    print(\"tfid: \", i)\n",
    "    print(\"tf: \",j)\n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> bag of words랑 index순서는 같음 <-> 값이 다름 (값이 클수록 희귀도가 높음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2번 단점 해결 대책 : Word2vec \"의미 유사도를 벡터로 생성\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "sentences=[d.split() for d in df['text']]   # 각 문장을 단어로 쪼갠 list 3개가 sentences로 들어감\n",
    "model = word2vec.Word2Vec(sentences, vector_size=10, min_count=1, window=2, seed=7)\n",
    "# 교재에는 size로 나와있으나 최신버전에서 vector_size로 변경됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01650858,  0.01069946,  0.00188946,  0.09910005,  0.06153275,\n",
       "        0.05853238,  0.04005488,  0.02443584, -0.03179482,  0.09779203],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['like']        # 벡터 형식으로 변환된 'like' 벡터 (10차원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 0.42540043592453003),\n",
       " ('machine', 0.36355969309806824),\n",
       " ('not', 0.311229407787323),\n",
       " ('kaggle', -0.004140517208725214),\n",
       " ('much', -0.11530755460262299),\n",
       " ('do', -0.1529018133878708),\n",
       " ('love', -0.2554278075695038),\n",
       " ('really', -0.4161785840988159),\n",
       " ('learning', -0.44330504536628723),\n",
       " ('very', -0.44338396191596985)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('like')   # 'like'단어 벡터와 유사한 벡터 순서대로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08898099,  0.02501909,  0.03683598,  0.07944275,  0.01565849,\n",
       "         0.05513714,  0.0667302 , -0.05495857, -0.08889369, -0.03996675],\n",
       "       [ 0.01650858,  0.01069946,  0.00188946,  0.09910005,  0.06153275,\n",
       "         0.05853238,  0.04005488,  0.02443584, -0.03179482,  0.09779203],\n",
       "       [ 0.06329302, -0.03939352, -0.03167932, -0.04431488,  0.04389417,\n",
       "        -0.04902608,  0.09809195, -0.01098474, -0.00437022,  0.00090965],\n",
       "       [ 0.03720424, -0.02774719,  0.02864924,  0.01963681, -0.07835456,\n",
       "        -0.08814968,  0.03203132, -0.02247364,  0.01966591, -0.03539274],\n",
       "       [-0.09157717,  0.04835419, -0.00529734, -0.08170088, -0.05110302,\n",
       "         0.00822875,  0.04535742,  0.00155444,  0.02258943,  0.07426786]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "wordvec = np.array([model.wv[word] for word in df['text'][0].split()])\n",
    "# text 데이터 프레임의 첫문장을 단어 단위로 쪼갬 -> 각 단어마다 10차원 벡터로 변환 -> wordvec이라는 배열에 추가\n",
    "wordvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> text 데이터 프레임의 첫문장의 단어들을 모두 10차원 벡터로 변환한 array wordvet생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터화한 단어들을 머신러닝 알고리즘에 입력\n",
    "\n",
    "1. 문장에 등장하는 단어 벡터의 평균값\n",
    "2. 문장에 등장하는 단어 벡터 각 요소의 최댓값\n",
    "3. 각 단어를 시계열 데이터로 다룸 \"RNN, BERT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 평균값>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02288193,  0.00338641,  0.0060796 ,  0.01443277, -0.00167443,\n",
       "       -0.0030555 ,  0.05645315, -0.01248533, -0.01656068,  0.01952201],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(wordvec, axis=0) #5개의 10차원 단어 벡터를 각 요소(차원)마다 평균값 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<최댓값> \"SWEM-max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08898099, 0.04835419, 0.03683598, 0.09910005, 0.06153275,\n",
       "       0.05853238, 0.09809195, 0.02443584, 0.02258943, 0.09779203],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(wordvec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
